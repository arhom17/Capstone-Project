{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e333f87d",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "This notebook contains the code for reading in and processing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96eae7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9064c987",
   "metadata": {},
   "source": [
    "First we will read in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cf1b042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read User Info Data\n",
    "user_info = pd.read_csv('data/raw/user_info.txt', sep='\\t')\n",
    "user_info = user_info[user_info['JOB_TITLE'].notna() & (user_info['JOB_TITLE'] != '')]\n",
    "\n",
    "# print(user_info.head(10))\n",
    "# print(len(user_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "429ad06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Tenant Info Data\n",
    "tenant_info = pd.read_csv('data/raw/tenant_info.txt', sep='\\t')\n",
    "\n",
    "# print(tenant_info.head(10))\n",
    "# print(len(tenant_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c1a93ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Interaction Data\n",
    "interactions = pd.read_csv('data/raw/interactions.txt', sep='\\t')\n",
    "\n",
    "# print(interactions.head(10))\n",
    "# print(len(interactions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52e1e452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/30/hz4jjfbx5nnd_ndmwg803m340000gn/T/ipykernel_52894/944382396.py:2: DtypeWarning: Columns (2,3,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  content_topics = pd.read_csv('data/raw/content_topics.txt', sep='\\t')\n"
     ]
    }
   ],
   "source": [
    "# Read Topic Data\n",
    "content_topics = pd.read_csv('data/raw/content_topics.txt', sep='\\t')\n",
    "content_topics = content_topics[content_topics['TOPIC'].notna() & (content_topics['TOPIC'] != '')]\n",
    "\n",
    "# print(content_topics.head(10))\n",
    "# print(len(content_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1c6f0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Tag Data\n",
    "content_tags = pd.read_csv('data/raw/content_tags.txt', sep='\\t')\n",
    "content_tags = content_tags[content_tags['TAG'].notna() & (content_tags['TAG'] != '')]\n",
    "\n",
    "# print(content_tags.head(10))\n",
    "# print(len(content_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4c7a8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Keywords Data\n",
    "content_keywords = pd.read_csv('data/raw/content_keywords.txt', sep='\\t')\n",
    "content_keywords = content_keywords[content_keywords['KEYWORD'].notna() & (content_keywords['KEYWORD'] != '')]\n",
    "\n",
    "# print(content_keywords.head(10))\n",
    "# print(len(content_keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449e5bbf",
   "metadata": {},
   "source": [
    "I will create and export samples for quick local viewing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c11a838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and Export Sample Sets\n",
    "user_info_sample = user_info.sample(n=100, random_state=42)\n",
    "tenant_info_sample = tenant_info.sample(n=100, random_state=42)\n",
    "interactions_sample = interactions.sample(n=100, random_state=42)\n",
    "content_topics_sample = content_topics.sample(n=100, random_state=42)\n",
    "content_tags_sample = content_tags.sample(n=100, random_state=42)\n",
    "content_keywords_sample = content_keywords.sample(n=100, random_state=42)\n",
    "\n",
    "# Export\n",
    "export_path = 'data/sample/'\n",
    "user_info_sample.to_csv(f'{export_path}user_info_sample.csv', index=False)\n",
    "tenant_info_sample.to_csv(f'{export_path}tenant_info_sample.csv', index=False)\n",
    "interactions_sample.to_csv(f'{export_path}interactions_sample.csv', index=False)\n",
    "content_topics_sample.to_csv(f'{export_path}content_topics_sample.csv', index=False)\n",
    "content_tags_sample.to_csv(f'{export_path}content_tags_sample.csv', index=False)\n",
    "content_keywords_sample.to_csv(f'{export_path}content_keywords_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ce57b1",
   "metadata": {},
   "source": [
    "Create aggregated lists of Topics, Tags, and Keywords for each piece of content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2514ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Topic Data\n",
    "content_topics_agg = content_topics.groupby('CONTENT_ID').agg({\n",
    "    'TOPIC': lambda x: ', '.join(x.dropna().astype(str).unique()),\n",
    "    'SUPTOPIC': lambda x: ', '.join(x.dropna().astype(str).unique()),\n",
    "    'TOPIC_SUBTOPIC': lambda x: ', '.join(x.dropna().astype(str).unique())\n",
    "}).reset_index()\n",
    "# Source: https://stackoverflow.com/questions/27298178/concatenate-strings-from-several-rows-using-pandas-groupby\n",
    "\n",
    "content_topics_agg.columns = ['CONTENT_ID', 'TOPICS', 'SUBTOPICS', 'TOPIC_SUBTOPICS']\n",
    "\n",
    "# print(content_topics_agg.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09c8446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Tag Data\n",
    "content_tags_agg = content_tags.groupby('CONTENT_ID').agg({\n",
    "    'TAG': lambda x: ', '.join(x.dropna().astype(str).unique())\n",
    "}).reset_index()\n",
    "\n",
    "content_tags_agg.columns = ['CONTENT_ID', 'TAGS']\n",
    "\n",
    "# print(content_tags_agg.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da14713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Keyword Data\n",
    "content_keywords_agg = content_keywords.groupby('CONTENT_ID').agg({\n",
    "    'KEYWORD': lambda x: ', '.join(x.dropna().astype(str).unique())\n",
    "}).reset_index()\n",
    "\n",
    "content_keywords_agg.columns = ['CONTENT_ID', 'KEYWORDS']\n",
    "\n",
    "# print(content_keywords_agg.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f88099",
   "metadata": {},
   "source": [
    "Export and Re-import if needed to save time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1aa3f690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Aggregated Content Data\n",
    "# export_path = 'data/processed/' \n",
    "# content_topics_agg.to_csv(f'{export_path}content_topics_agg.csv', index=False)\n",
    "# content_tags_agg.to_csv(f'{export_path}content_tags_agg.csv', index=False)\n",
    "# content_keywords_agg.to_csv(f'{export_path}content_keywords_agg.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f981c66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Aggregated Content Data\n",
    "# content_topics_agg = pd.read_csv('data/processed/content_topics_agg.csv', sep=',')\n",
    "# content_tags_agg = pd.read_csv('data/processed/content_tags_agg.csv', sep=',')\n",
    "# content_keywords_agg = pd.read_csv('data/processed/content_keywords_agg.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac048a4f",
   "metadata": {},
   "source": [
    "Aggregate the grouped topics, tags, and keywords data into interactions. Join Tenant and User info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2891d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and Aggregate Interactions Data\n",
    "interactions_agg = interactions.copy()\n",
    "\n",
    "interactions_agg = interactions_agg.merge(\n",
    "    user_info, \n",
    "    how='left',\n",
    "    left_on='USER_ID', \n",
    "    right_on='USERID'\n",
    ").drop('USERID', axis=1)  # Drop duplicate ID\n",
    "\n",
    "interactions_agg = interactions_agg.merge(\n",
    "    tenant_info, \n",
    "    how='left',\n",
    "    on='TENANT_ID'\n",
    ")\n",
    "\n",
    "interactions_agg = interactions_agg.merge(\n",
    "    content_topics_agg, \n",
    "    how='left',\n",
    "    on='CONTENT_ID'\n",
    ")\n",
    "\n",
    "\n",
    "interactions_agg = interactions_agg.merge(\n",
    "    content_tags_agg, \n",
    "    how='left',\n",
    "    on='CONTENT_ID'\n",
    ")\n",
    "\n",
    "interactions_agg = interactions_agg.merge(\n",
    "    content_keywords_agg, \n",
    "    how='left',\n",
    "    on='CONTENT_ID'\n",
    ")\n",
    "# Source: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html\n",
    "\n",
    "# print(interactions_agg.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9601b3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (3727152, 14)\n",
      "Columns: ['INTERACTION_ID', 'INTERACTION_DATE', 'INTERACTION_TYPE', 'TENANT_ID', 'USER_ID', 'CONTENT_ID', 'JOB_TITLE', 'INDUSTRY', 'SEGMENT', 'TOPICS', 'SUBTOPICS', 'TOPIC_SUBTOPICS', 'TAGS', 'KEYWORDS']\n",
      "\n",
      "\n",
      "Aggregation Stats:\n",
      "INDUSTRY: 84.7% filled\n",
      "SEGMENT: 91.9% filled\n",
      "JOB_TITLE: 23.9% filled\n",
      "TOPICS: 94.2% filled\n",
      "SUBTOPICS: 89.5% filled\n",
      "TOPIC_SUBTOPICS: 94.2% filled\n",
      "TAGS: 17.3% filled\n",
      "KEYWORDS: 38.7% filled\n"
     ]
    }
   ],
   "source": [
    "# Display Shape and Columns\n",
    "print(f\"Shape: {interactions_agg.shape}\")\n",
    "print(f\"Columns: {list(interactions_agg.columns)}\")\n",
    "\n",
    "\n",
    "# Show Statistics\n",
    "print(\"\\n\")\n",
    "print(\"Aggregation Stats:\")\n",
    "cols = ['INDUSTRY', 'SEGMENT', 'JOB_TITLE', 'TOPICS', 'SUBTOPICS', 'TOPIC_SUBTOPICS', 'TAGS', 'KEYWORDS']\n",
    "for col in cols:\n",
    "    non_empty = interactions_agg[col].str.len() > 0\n",
    "    print(f\"{col}: {non_empty.mean():.1%} filled\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff95350",
   "metadata": {},
   "source": [
    "Since job title, tags, and keywords aren't heavily populated, we will drop them. We will also drop subtopics as we will only use them with the combined topics_subtopics field.I'll also pull out 10 random users to use for the validation of recommendations after the models are trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffe1f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Subset and Drop JOB_TITLE, TAGS, and KEYWORDS\n",
    "interactions_agg = interactions_agg[['INTERACTION_ID', 'INTERACTION_DATE', 'INTERACTION_TYPE', 'TENANT_ID', 'USER_ID', 'CONTENT_ID', 'INDUSTRY', 'SEGMENT', 'TOPICS', 'TOPIC_SUBTOPICS']]\n",
    "\n",
    "# Select 10 random users for validation set\n",
    "random_users = interactions_agg['USER_ID'].drop_duplicates().sample(n=10, random_state=42)\n",
    "\n",
    "# Create validation set with all interactions from these 10 users\n",
    "interactions_agg_val = interactions_agg[interactions_agg['USER_ID'].isin(random_users)]\n",
    "\n",
    "# Remove these users from the main dataset\n",
    "interactions_agg = interactions_agg[~interactions_agg['USER_ID'].isin(random_users)]\n",
    "\n",
    "# Export dataset\n",
    "export_path = 'data/processed/' \n",
    "interactions_agg.to_csv(f'{export_path}interactions_agg.csv', index=False)\n",
    "interactions_agg_val.to_csv(f'{export_path}interactions_agg_val.csv', index=False)\n",
    "\n",
    "# print(interactions_agg.head(10))\n",
    "\n",
    "# Create and Export Sample Set\n",
    "interactions_agg_sample = interactions_agg.sample(n=100, random_state=42)\n",
    "\n",
    "# Export\n",
    "export_path = 'data/sample/'\n",
    "interactions_agg_sample.to_csv(f'{export_path}interactions_agg_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373def1e",
   "metadata": {},
   "source": [
    "For the purpose of content recommendations, it might be better to train on a list without aggregated values. I will create a 'long' dataset with an entry for each aggregate combo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc598c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'Long' Dataset\n",
    "interactions_agg_long = interactions.copy()\n",
    "\n",
    "interactions_agg_long = interactions_agg_long.merge(\n",
    "    user_info, \n",
    "    how='left',\n",
    "    left_on='USER_ID', \n",
    "    right_on='USERID',\n",
    ").drop('USERID', axis=1)  # Drop duplicate ID\n",
    "\n",
    "interactions_agg_long = interactions_agg_long.merge(\n",
    "    tenant_info, \n",
    "    how='left',\n",
    "    on='TENANT_ID'\n",
    ")\n",
    "\n",
    "interactions_agg_long = interactions_agg_long.merge(\n",
    "    content_topics, \n",
    "    how='left',\n",
    "    on='CONTENT_ID'\n",
    ")\n",
    "\n",
    "# Grab relevant columns\n",
    "interactions_agg_long = interactions_agg_long[['INTERACTION_ID', 'INTERACTION_DATE', 'INTERACTION_TYPE', 'TENANT_ID', 'USER_ID', 'CONTENT_ID', 'INDUSTRY', 'SEGMENT', 'TOPIC', 'TOPIC_SUBTOPIC']]\n",
    "\n",
    "# Remove rows with missing data\n",
    "interactions_agg_long = interactions_agg_long[\n",
    "    interactions_agg_long['INDUSTRY'].notna() & \n",
    "    interactions_agg_long['SEGMENT'].notna() & \n",
    "    interactions_agg_long['TOPIC'].notna() & \n",
    "    interactions_agg_long['TOPIC_SUBTOPIC'].notna()\n",
    "]\n",
    "# Source: https://www.geeksforgeeks.org/python/drop-rows-from-pandas-dataframe-with-missing-values-or-nan-in-columns/\n",
    "\n",
    "# Replace 'Engineering' and 'Architecture' with 'Multidiscipline (Arch & Eng)'\n",
    "interactions_agg_long['INDUSTRY'] = interactions_agg_long['INDUSTRY'].replace(['Engineering', 'Architecture'], 'Multidiscipline (Arch & Eng)')\n",
    "\n",
    "# Remove rows where INDUSTRY is 'Unknown'\n",
    "interactions_agg_long = interactions_agg_long[interactions_agg_long['INDUSTRY'] != 'Unknown']\n",
    "\n",
    "# Create validation set with the same 10 users\n",
    "interactions_agg_long_val = interactions_agg_long[interactions_agg_long['USER_ID'].isin(random_users)]\n",
    "\n",
    "# Remove these users from the main long dataset\n",
    "interactions_agg_long = interactions_agg_long[~interactions_agg_long['USER_ID'].isin(random_users)]\n",
    "\n",
    "# Export\n",
    "export_path = 'data/processed/'\n",
    "interactions_agg_long.to_csv(f'{export_path}interactions_agg_long.csv', index=False)\n",
    "interactions_agg_long_val.to_csv(f'{export_path}interactions_agg_long_val.csv', index=False)\n",
    "\n",
    "# print(interactions_agg_long.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d03d87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (4919195, 10)\n",
      "Columns: ['INTERACTION_ID', 'INTERACTION_DATE', 'INTERACTION_TYPE', 'TENANT_ID', 'USER_ID', 'CONTENT_ID', 'INDUSTRY', 'SEGMENT', 'TOPIC', 'TOPIC_SUBTOPIC']\n",
      "\n",
      "\n",
      "Aggregation Stats:\n",
      "INDUSTRY: 100.0% filled\n",
      "SEGMENT: 100.0% filled\n",
      "TOPIC: 100.0% filled\n",
      "TOPIC_SUBTOPIC: 100.0% filled\n"
     ]
    }
   ],
   "source": [
    "# Display Shape and Columns\n",
    "print(f\"Shape: {interactions_agg_long.shape}\")\n",
    "print(f\"Columns: {list(interactions_agg_long.columns)}\")\n",
    "\n",
    "\n",
    "# Show Statistics\n",
    "print(\"\\n\")\n",
    "print(\"Aggregation Stats:\")\n",
    "cols = ['INDUSTRY', 'SEGMENT', 'TOPIC', 'TOPIC_SUBTOPIC']\n",
    "for col in cols:\n",
    "    non_empty = interactions_agg_long[col].str.len() > 0\n",
    "    print(f\"{col}: {non_empty.mean():.1%} filled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0eb60bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and Export Sample Set\n",
    "interactions_agg_long_sample = interactions_agg_long.sample(n=100, random_state=42)\n",
    "\n",
    "# Export Sample\n",
    "export_path = 'data/sample/'\n",
    "interactions_agg_long_sample.to_csv(f'{export_path}interactions_agg_long_sample.csv', index=False)\n",
    "\n",
    "# Export Full\n",
    "export_path = 'data/processed/'\n",
    "interactions_agg_long.to_csv(f'{export_path}interactions_agg_long.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff359405",
   "metadata": {},
   "source": [
    "We will encode each of the values for use later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "945bc64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_path = 'data/processed/'\n",
    "\n",
    "# INDUSTRY Encoding\n",
    "industry_unique = interactions_agg_long['INDUSTRY'].dropna().unique()\n",
    "industry_le = LabelEncoder()\n",
    "industry_encoded = industry_le.fit_transform(industry_unique)\n",
    "industry_df = pd.DataFrame({\n",
    "    'INDUSTRY': industry_unique,\n",
    "    'INDUSTRY_encoded': industry_encoded\n",
    "})\n",
    "industry_df = industry_df.sort_values(by='INDUSTRY').reset_index(drop=True)\n",
    "industry_df.to_csv(f'{export_path}industry_enc.csv', index=False)\n",
    "\n",
    "# SEGMENT Encoding\n",
    "segment_unique = interactions_agg_long['SEGMENT'].dropna().unique()\n",
    "segment_le = LabelEncoder()\n",
    "segment_encoded = segment_le.fit_transform(segment_unique)\n",
    "segment_df = pd.DataFrame({\n",
    "    'SEGMENT': segment_unique,\n",
    "    'SEGMENT_encoded': segment_encoded\n",
    "})\n",
    "segment_df = segment_df.sort_values(by='SEGMENT').reset_index(drop=True)\n",
    "segment_df.to_csv(f'{export_path}segment_enc.csv', index=False)\n",
    "\n",
    "# TOPIC Encoding\n",
    "topic_unique = interactions_agg_long['TOPIC'].dropna().unique()\n",
    "topic_le = LabelEncoder()\n",
    "topic_encoded = topic_le.fit_transform(topic_unique)\n",
    "topic_df = pd.DataFrame({\n",
    "    'TOPIC': topic_unique,\n",
    "    'TOPIC_encoded': topic_encoded\n",
    "})\n",
    "topic_df = topic_df.sort_values(by='TOPIC').reset_index(drop=True)\n",
    "topic_df.to_csv(f'{export_path}topic_enc.csv', index=False)\n",
    "\n",
    "# TOPIC_SUBTOPIC Encoding\n",
    "topic_subtopic_unique = interactions_agg_long['TOPIC_SUBTOPIC'].dropna().unique()\n",
    "topic_subtopic_le = LabelEncoder()\n",
    "topic_subtopic_encoded = topic_subtopic_le.fit_transform(topic_subtopic_unique)\n",
    "topic_subtopic_df = pd.DataFrame({\n",
    "    'TOPIC_SUBTOPIC': topic_subtopic_unique,\n",
    "    'TOPIC_SUBTOPIC_encoded': topic_subtopic_encoded\n",
    "})\n",
    "topic_subtopic_df = topic_subtopic_df.sort_values(by='TOPIC_SUBTOPIC').reset_index(drop=True)\n",
    "topic_subtopic_df.to_csv(f'{export_path}topic_subtopic_enc.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28542c6",
   "metadata": {},
   "source": [
    "Create KNN Dataset: \n",
    "This dataset will have a row for every user, and a column for each topic. The intersect will be the interaction precentages (num_interactions / total_interactions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898ce73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the two DataFrames for complete dataset, then pull out 10 users as the end\n",
    "interactions_knn= pd.concat([interactions_agg_long, interactions_agg_long_val])\n",
    "\n",
    "\n",
    "# Group by user\n",
    "grouped_users = interactions_knn.groupby('USER_ID').agg({\n",
    "    'INDUSTRY': 'first',\n",
    "    'SEGMENT': 'first',\n",
    "    'TENANT_ID': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "# Map encoded values\n",
    "grouped_users = grouped_users.merge(\n",
    "    industry_df[['INDUSTRY', 'INDUSTRY_encoded']], \n",
    "    on='INDUSTRY', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "grouped_users = grouped_users.merge(\n",
    "    segment_df[['SEGMENT', 'SEGMENT_encoded']], \n",
    "    on='SEGMENT', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Order columns\n",
    "grouped_users = grouped_users[['USER_ID', 'TENANT_ID', 'INDUSTRY', 'INDUSTRY_encoded', 'SEGMENT', 'SEGMENT_encoded']]\n",
    "\n",
    "# Count interactions per user\n",
    "topic_pivot = interactions_knn.groupby(['USER_ID', 'TOPIC']).size().unstack(fill_value=0)\n",
    "# NOTICE: Claude assisted me with this line of code. I was originally trying a loop, but this wasn't working. Claude suggested group_by + .size()\n",
    "\n",
    "# Grab topic column names\n",
    "topic_cols = list(topic_pivot.columns)\n",
    "\n",
    "# Calculate total interactions per user\n",
    "total_interactions = topic_pivot.sum(axis=1).replace(0, 1) # Added replace to div 0 errors\n",
    "\n",
    "# Convert counts to percentages\n",
    "for col in topic_cols:\n",
    "    topic_pivot[col] = (topic_pivot[col] / total_interactions * 100).round(2)\n",
    "\n",
    "# Merge grouped users with topic percentages\n",
    "knn_dataset = grouped_users.merge(\n",
    "    topic_pivot, \n",
    "    how='left',\n",
    "    on='USER_ID'\n",
    ")\n",
    "\n",
    "# Fill null values with 0\n",
    "knn_dataset[topic_cols] = knn_dataset[topic_cols].fillna(0)\n",
    "\n",
    "# Configure validation set with the 10 random users\n",
    "knn_dataset_val = knn_dataset[knn_dataset['USER_ID'].isin(random_users)]\n",
    "knn_dataset = knn_dataset[~knn_dataset['USER_ID'].isin(random_users)]\n",
    "\n",
    "# print(knn_dataset.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9515ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export\n",
    "export_path = 'data/processed/'\n",
    "knn_dataset.to_csv(f'{export_path}knn_dataset.csv', index=False)\n",
    "knn_dataset_val.to_csv(f'{export_path}knn_dataset_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714924c6",
   "metadata": {},
   "source": [
    "Create Neural Network Dataset:\n",
    "Each row in this dataset will be a collection of 5 sequential content interactions for a user. The interactions will be represented by their topic_subtopic, with a target for classification. The actual content_id of the target will also be included as this will be used for judging recommendation accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaea86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network_set = interactions_agg_long.copy()\n",
    "\n",
    "# Convert date to datetime and sort\n",
    "neural_network_set['INTERACTION_DATE'] = pd.to_datetime(neural_network_set['INTERACTION_DATE'])\n",
    "neural_newtork_sorted = neural_network_set.sort_values(['USER_ID', 'INTERACTION_DATE']).reset_index(drop=True)\n",
    "\n",
    "# Function to create sequences for a single user\n",
    "def create_user_sequences(user_data, sequence_length):\n",
    "    \n",
    "    user_data = user_data.reset_index(drop=True)\n",
    "    \n",
    "    # Return nothing is user doesn't have enough data\n",
    "    if len(user_data) < sequence_length + 1:\n",
    "        return []\n",
    "    \n",
    "    # Grab user's tenant, industry, and segment\n",
    "    user_context = {\n",
    "        'tenant_id': user_data['TENANT_ID'].iloc[0], # Grab first value\n",
    "        'industry': user_data['INDUSTRY'].iloc[0],\n",
    "        'segment': user_data['SEGMENT'].iloc[0]\n",
    "    }\n",
    "    \n",
    "    # Grab topic_subtopic, content_id, and topic\n",
    "    topic_subtopics = user_data['TOPIC_SUBTOPIC'].values # Grab all values\n",
    "    content_ids = user_data['CONTENT_ID'].values\n",
    "    topics = user_data['TOPIC'].values\n",
    "    \n",
    "    # Sliding window\n",
    "    sequences = []\n",
    "    for i in range(len(topic_subtopics) - sequence_length): # How many sequences\n",
    "        sequence_dict = {'user_id': user_data['USER_ID'].iloc[0]}\n",
    "        sequence_dict.update(user_context)\n",
    "        \n",
    "        # Build sequence\n",
    "        for j in range(sequence_length):\n",
    "            sequence_dict[f'topic_subtopic_{j+1}'] = topic_subtopics[i + j]\n",
    "        \n",
    "        # Target fields, what comes after sequence\n",
    "        sequence_dict['target_topic_subtopic'] = topic_subtopics[i + sequence_length]\n",
    "        sequence_dict['target_content_id'] = content_ids[i + sequence_length]\n",
    "        sequence_dict['target_topic'] = topics[i + sequence_length]\n",
    "        \n",
    "        sequences.append(sequence_dict)\n",
    "    # Source: https://www.geeksforgeeks.org/dsa/window-sliding-technique/\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "# Process each user and create sequences\n",
    "all_sequences = []\n",
    "for user_id, user_group in neural_newtork_sorted.groupby('USER_ID'):\n",
    "    user_sequences = create_user_sequences(user_group, 5) # sequence length of 5\n",
    "    all_sequences.extend(user_sequences)  \n",
    "\n",
    "# Create DF from sequences\n",
    "neural_network_dataset = pd.DataFrame(all_sequences)\n",
    "\n",
    "# Store the original target_topic as target_topic_eng before encoding\n",
    "neural_network_dataset['target_topic_eng'] = neural_network_dataset['target_topic'].copy()\n",
    "\n",
    "# Map encoded values - NOTICE: Claude helped me implement this code. I knew conceptually what I wanted to achieve (as done so for the KNN dataset), Claude gave me the skeleton to incorporate here.\n",
    "# Industry\n",
    "neural_network_dataset = neural_network_dataset.merge(\n",
    "    industry_df[['INDUSTRY', 'INDUSTRY_encoded']],\n",
    "    left_on='industry',\n",
    "    right_on='INDUSTRY',\n",
    "    how='left'\n",
    ").drop(columns=['INDUSTRY'])\n",
    "neural_network_dataset['industry'] = neural_network_dataset['INDUSTRY_encoded']\n",
    "neural_network_dataset = neural_network_dataset.drop(columns=['INDUSTRY_encoded'])\n",
    "\n",
    "# Segment\n",
    "neural_network_dataset = neural_network_dataset.merge(\n",
    "    segment_df[['SEGMENT', 'SEGMENT_encoded']],\n",
    "    left_on='segment',\n",
    "    right_on='SEGMENT',\n",
    "    how='left'\n",
    ").drop(columns=['SEGMENT'])\n",
    "neural_network_dataset['segment'] = neural_network_dataset['SEGMENT_encoded']\n",
    "neural_network_dataset = neural_network_dataset.drop(columns=['SEGMENT_encoded'])\n",
    "\n",
    "# Topic_subtopic columns (1-5 and target)\n",
    "for i in range(1, 6):\n",
    "    col_name = f'topic_subtopic_{i}'\n",
    "    neural_network_dataset = neural_network_dataset.merge(\n",
    "        topic_subtopic_df[['TOPIC_SUBTOPIC', 'TOPIC_SUBTOPIC_encoded']],\n",
    "        left_on=col_name,\n",
    "        right_on='TOPIC_SUBTOPIC',\n",
    "        how='left',\n",
    "        suffixes=('', f'_{i}')\n",
    "    ).drop(columns=['TOPIC_SUBTOPIC'])\n",
    "    neural_network_dataset[col_name] = neural_network_dataset['TOPIC_SUBTOPIC_encoded']\n",
    "    neural_network_dataset = neural_network_dataset.drop(columns=['TOPIC_SUBTOPIC_encoded'])\n",
    "\n",
    "# Target_topic_subtopic\n",
    "neural_network_dataset = neural_network_dataset.merge(\n",
    "    topic_subtopic_df[['TOPIC_SUBTOPIC', 'TOPIC_SUBTOPIC_encoded']],\n",
    "    left_on='target_topic_subtopic',\n",
    "    right_on='TOPIC_SUBTOPIC',\n",
    "    how='left'\n",
    ").drop(columns=['TOPIC_SUBTOPIC'])\n",
    "neural_network_dataset['target_topic_subtopic'] = neural_network_dataset['TOPIC_SUBTOPIC_encoded']\n",
    "neural_network_dataset = neural_network_dataset.drop(columns=['TOPIC_SUBTOPIC_encoded'])\n",
    "\n",
    "# Target_topic\n",
    "neural_network_dataset = neural_network_dataset.merge(\n",
    "    topic_df[['TOPIC', 'TOPIC_encoded']],\n",
    "    left_on='target_topic',\n",
    "    right_on='TOPIC',\n",
    "    how='left'\n",
    ").drop(columns=['TOPIC'])\n",
    "neural_network_dataset['target_topic'] = neural_network_dataset['TOPIC_encoded']\n",
    "neural_network_dataset = neural_network_dataset.drop(columns=['TOPIC_encoded'])\n",
    "\n",
    "# print(neural_network_dataset.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "de0587cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export\n",
    "export_path = 'data/processed/'\n",
    "neural_network_dataset.to_csv(f'{export_path}neural_network_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ab14f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and Export Sample Set\n",
    "knn_dataset_sample = knn_dataset.sample(n=100, random_state=42)\n",
    "neural_network_dataset_sample = neural_network_dataset.sample(n=100, random_state=42)\n",
    "\n",
    "# Export\n",
    "export_path = 'data/sample/'\n",
    "knn_dataset_sample.to_csv(f'{export_path}knn_dataset_sample.csv', index=False)\n",
    "neural_network_dataset_sample.to_csv(f'{export_path}neural_network_dataset_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f641409c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for Validation dataset\n",
    "neural_network_set_val = interactions_agg_long_val.copy()\n",
    "\n",
    "# Convert date to datetime and sort\n",
    "neural_network_set_val['INTERACTION_DATE'] = pd.to_datetime(neural_network_set_val['INTERACTION_DATE'])\n",
    "neural_newtork_sorted_val = neural_network_set_val.sort_values(['USER_ID', 'INTERACTION_DATE']).reset_index(drop=True)\n",
    "\n",
    "# Function to create sequences for a single user\n",
    "def create_user_sequences(user_data, sequence_length):\n",
    "    \n",
    "    user_data = user_data.reset_index(drop=True)\n",
    "    \n",
    "    # Return nothing is user doesn't have enough data\n",
    "    if len(user_data) < sequence_length + 1:\n",
    "        return []\n",
    "    \n",
    "    # Grab user's tenant, industry, and segment\n",
    "    user_context = {\n",
    "        'tenant_id': user_data['TENANT_ID'].iloc[0], # Grab first value\n",
    "        'industry': user_data['INDUSTRY'].iloc[0],\n",
    "        'segment': user_data['SEGMENT'].iloc[0]\n",
    "    }\n",
    "    \n",
    "    # Grab topic_subtopic, content_id, and topic\n",
    "    topic_subtopics = user_data['TOPIC_SUBTOPIC'].values # Grab all values\n",
    "    content_ids = user_data['CONTENT_ID'].values\n",
    "    topics = user_data['TOPIC'].values\n",
    "    \n",
    "    # Sliding window\n",
    "    sequences = []\n",
    "    for i in range(len(topic_subtopics) - sequence_length): # How many sequences\n",
    "        sequence_dict = {'user_id': user_data['USER_ID'].iloc[0]}\n",
    "        sequence_dict.update(user_context)\n",
    "        \n",
    "        # Build sequence\n",
    "        for j in range(sequence_length):\n",
    "            sequence_dict[f'topic_subtopic_{j+1}'] = topic_subtopics[i + j]\n",
    "        \n",
    "        # Target fields, what comes after sequence\n",
    "        sequence_dict['target_topic_subtopic'] = topic_subtopics[i + sequence_length]\n",
    "        sequence_dict['target_content_id'] = content_ids[i + sequence_length]\n",
    "        sequence_dict['target_topic'] = topics[i + sequence_length]\n",
    "        \n",
    "        sequences.append(sequence_dict)\n",
    "    # Source: https://www.geeksforgeeks.org/dsa/window-sliding-technique/\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "# Process each user and create sequences\n",
    "all_sequences_val = []\n",
    "for user_id, user_group in neural_newtork_sorted_val.groupby('USER_ID'):\n",
    "    user_sequences = create_user_sequences(user_group, 5) # sequence length of 5\n",
    "    all_sequences_val.extend(user_sequences)  \n",
    "\n",
    "# Create DF from sequences\n",
    "neural_network_dataset_val = pd.DataFrame(all_sequences_val)\n",
    "\n",
    "# Store the original target_topic as target_topic_eng before encoding\n",
    "neural_network_dataset_val['target_topic_eng'] = neural_network_dataset_val['target_topic'].copy()\n",
    "\n",
    "# Map encoded values - NOTICE: Claude helped me implement this code. I knew conceptually what I wanted to achieve (as done so for the KNN dataset), Claude gave me the skeleton to incorporate here.\n",
    "# Industry\n",
    "neural_network_dataset_val = neural_network_dataset_val.merge(\n",
    "    industry_df[['INDUSTRY', 'INDUSTRY_encoded']],\n",
    "    left_on='industry',\n",
    "    right_on='INDUSTRY',\n",
    "    how='left'\n",
    ").drop(columns=['INDUSTRY'])\n",
    "neural_network_dataset_val['industry'] = neural_network_dataset_val['INDUSTRY_encoded']\n",
    "neural_network_dataset_val = neural_network_dataset_val.drop(columns=['INDUSTRY_encoded'])\n",
    "\n",
    "# Segment\n",
    "neural_network_dataset_val = neural_network_dataset_val.merge(\n",
    "    segment_df[['SEGMENT', 'SEGMENT_encoded']],\n",
    "    left_on='segment',\n",
    "    right_on='SEGMENT',\n",
    "    how='left'\n",
    ").drop(columns=['SEGMENT'])\n",
    "neural_network_dataset_val['segment'] = neural_network_dataset_val['SEGMENT_encoded']\n",
    "neural_network_dataset_val = neural_network_dataset_val.drop(columns=['SEGMENT_encoded'])\n",
    "\n",
    "# Topic_subtopic columns (1-5 and target)\n",
    "for i in range(1, 6):\n",
    "    col_name = f'topic_subtopic_{i}'\n",
    "    neural_network_dataset_val = neural_network_dataset_val.merge(\n",
    "        topic_subtopic_df[['TOPIC_SUBTOPIC', 'TOPIC_SUBTOPIC_encoded']],\n",
    "        left_on=col_name,\n",
    "        right_on='TOPIC_SUBTOPIC',\n",
    "        how='left',\n",
    "        suffixes=('', f'_{i}')\n",
    "    ).drop(columns=['TOPIC_SUBTOPIC'])\n",
    "    neural_network_dataset_val[col_name] = neural_network_dataset_val['TOPIC_SUBTOPIC_encoded']\n",
    "    neural_network_dataset_val = neural_network_dataset_val.drop(columns=['TOPIC_SUBTOPIC_encoded'])\n",
    "\n",
    "# Target_topic_subtopic\n",
    "neural_network_dataset_val = neural_network_dataset_val.merge(\n",
    "    topic_subtopic_df[['TOPIC_SUBTOPIC', 'TOPIC_SUBTOPIC_encoded']],\n",
    "    left_on='target_topic_subtopic',\n",
    "    right_on='TOPIC_SUBTOPIC',\n",
    "    how='left'\n",
    ").drop(columns=['TOPIC_SUBTOPIC'])\n",
    "neural_network_dataset_val['target_topic_subtopic'] = neural_network_dataset_val['TOPIC_SUBTOPIC_encoded']\n",
    "neural_network_dataset_val = neural_network_dataset_val.drop(columns=['TOPIC_SUBTOPIC_encoded'])\n",
    "\n",
    "# Target_topic\n",
    "neural_network_dataset_val = neural_network_dataset_val.merge(\n",
    "    topic_df[['TOPIC', 'TOPIC_encoded']],\n",
    "    left_on='target_topic',\n",
    "    right_on='TOPIC',\n",
    "    how='left'\n",
    ").drop(columns=['TOPIC'])\n",
    "neural_network_dataset_val['target_topic'] = neural_network_dataset_val['TOPIC_encoded']\n",
    "neural_network_dataset_val = neural_network_dataset_val.drop(columns=['TOPIC_encoded'])\n",
    "\n",
    "# Export\n",
    "export_path = 'data/processed/'\n",
    "neural_network_dataset_val.to_csv(f'{export_path}neural_network_dataset_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147322fa",
   "metadata": {},
   "source": [
    "Split knn_dataset and neural_network_dataset into test and train:\n",
    "* Update, this is now being done in the model_training.ipynb file due to some additional feature engineering that takes place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dc3819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Basic 80/20 Split\n",
    "# knn_train, knn_test = train_test_split(knn_dataset, test_size=0.2, random_state=42,)\n",
    "# neural_network_train, neural_network_test = train_test_split(neural_network_dataset, test_size=0.2, random_state=42,)\n",
    "# # Source: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "# # Export\n",
    "# export_path = 'data/processed/'\n",
    "# knn_train.to_csv(f'{export_path}knn_train.csv', index=False)\n",
    "# knn_test.to_csv(f'{export_path}knn_test.csv', index=False)\n",
    "# neural_network_train.to_csv(f'{export_path}neural_network_train.csv', index=False)\n",
    "# neural_network_test.to_csv(f'{export_path}neural_network_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
